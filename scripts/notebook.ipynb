{
  "cells": [
    {
      "metadata": {
        "_uuid": "095d45953e791275bbc47f711bb0c00d3682dc5d",
        "_cell_guid": "0cb3e036-7ff4-4e1f-94c3-82335eebedf8",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.neural_network import MLPClassifier\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "sample_submission.csv\ntest.csv\ntrain.csv\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "a4c30f65ee8c7005656cf3e15390e58c76eefc2b",
        "_cell_guid": "bf7961a5-b9e0-471b-8a3a-ff5e177b0026",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\nfrom scipy.special import logit, expit\nfrom tqdm import tqdm\n\nclass_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\ntrain = pd.read_csv('../input/train.csv').fillna(' ')\ntest = pd.read_csv('../input/test.csv').fillna(' ')\n\ntrain_text = train['comment_text']\ntest_text = test['comment_text']\n\ndef correlation(a , b ,unwanted='*'):\n\ta = list(a.lower())\n\tb = list(b.lower())\n\t\n\tcount = 0 \n\tflag =0\n\tfor i , j in zip(a,b) :\n\t\tif (not flag) and j not in unwanted:\n\t\t\tflag =1\n\t\tif i == j or j in unwanted :\n\t\t\tcount = count + 1\n\t\t\tcontinue\n\n\t\treturn 0\n\treturn count >= len(a)/2 and flag\nprint(train.head())",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \n0             0        0       0       0              0  \n1             0        0       0       0              0  \n2             0        0       0       0              0  \n3             0        0       0       0              0  \n4             0        0       0       0              0  \n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "collapsed": true,
        "_uuid": "0d8044f3407e43aa573e87121dd7461bf5f430e4",
        "_cell_guid": "87a45d32-83e4-4dbc-91f4-b1e2f437f111",
        "trusted": false
      },
      "cell_type": "code",
      "source": "\ndef remove_unwanted_char(arr):\n    badWords = [\"fuck\" ,\"cock\" ,\"suck\" ,\"piss\" ,\"bullshit\" ,\"ass\" ,\"asshole\" ,\"dick\",\"shit\" ,\"motherfuck\"]\n    for k in range(len(arr)) :\n        tmp = arr[k].split()\n        flag = 0\n        for j in badWords:\n            for i in range(len(tmp)):\n                if correlation(j ,tmp[i]) :\n                    tmp[i]= j\n                    flag =1\n                    \n        if flag :\n            arr[k] = \" \".join(tmp)\n    return arr\n#train_text = remove_unwanted_char(train_text)\n#test_text = remove_unwanted_char(test_text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "7610c24f0f603aa8831fb002a11f84ce261188cf",
        "_cell_guid": "efb11137-545b-456a-917b-96175150a956",
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(np.shape([train_text, test_text]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "b4d35aa85e442b3fcf36abcefe1f21ec0bbf18ff",
        "_cell_guid": "45e45260-6471-4455-8df8-72bd1d991246",
        "trusted": true
      },
      "cell_type": "code",
      "source": "\n\nall_text = pd.concat([train_text, test_text])\n \n\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    ngram_range=(1, 5),\n    max_features=30000)\nchar_vectorizer.fit(all_text)\ntrain_char_features = char_vectorizer.transform(train_text)\ntest_char_features = char_vectorizer.transform(test_text)\n\n\n\nlosses = []\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}.',\n    ngram_range=(1, 1),\n    max_features=40000,\n    max_df =.4 ,\n    min_df    = .000001\n)\n#word_vectorizer.fit(all_text)\n#train_word_features = word_vectorizer.transform(train_text)\n#test_word_features = word_vectorizer.transform(test_text)\ntrain_features = hstack([train_char_features])\ntest_features = hstack([test_char_features])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e6d4ced19a72edad4237306241ce0cf4a95bac6f",
        "_cell_guid": "9f3bd816-d8fa-4881-9ac0-c484ca2f1a22",
        "trusted": true
      },
      "cell_type": "code",
      "source": "\npredictions = {'id': test['id']}\nfor class_name in tqdm(class_names):\n#     if class_name == \"threat\" :\n        \n    train_target = train[class_name]\n    #classifier = LogisticRegression(solver='sag',multi_class=\"ovr\",tol=1e-10)\n    classifier =MLPClassifier(solver='lbfgs', alpha=1e-5,      hidden_layer_sizes=(15,), random_state=1)\n    cv_loss = np.mean(cross_val_score(classifier,train_features, train_target, cv=3, scoring='roc_auc'))\n    losses.append(cv_loss)\n    print('CV score for class {} is {}'.format(class_name, cv_loss))\n\n    classifier.fit(train_features, train_target)\n    predictions[class_name] = classifier.predict_proba(test_features)[:, 1]\n    \nprint('Total CV score is {}'.format(np.mean(losses)))\nsubmission = pd.DataFrame.from_dict(predictions)\nsubmission.to_csv('submission.csv', index=False)\n\n ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "_uuid": "8fad495da0cf5dfbe5408e0a6dbaabce1f9b582e",
        "_cell_guid": "5597d841-d70d-4e2f-a006-30857a10570a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "print((train_features.toarray))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "0a6233c82678f928c3a64ec16c9a8ae350a2168b",
        "_cell_guid": "e5634c32-7cbb-449e-bc7c-5b4d70df614a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# from scipy import sparse\n# sparse.save_npz('train_featureMatrix.npz', train_features)\n# sparse.save_npz('test_featureMatrix.npz', test_features)\n\n# #Load\n# #data = sparse.load_npz(\"data_sparse.npz\")\n# np.savetxt('train_featureMatrix.csv',train_features,delimiter=',' , fmt='%.10f' )\n# np.savetxt('test_featureMatrix.csv',test_features,delimiter=',' , fmt='%.10f' )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "65beb85d4f8c37f362c591ae888e2e908c225740",
        "_cell_guid": "7037c91f-1594-41aa-963c-7d90b5547fcc",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "f0f9fd11a4fec4176fbaa0a4746d850320ba0916",
        "_cell_guid": "24efe316-3107-4e1e-b4e7-2eb87d12f93e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "361d061758157898aeeae3d35a4cb40df043d659",
        "_cell_guid": "4f9b2074-1cdd-4091-bb93-45a1b6c32898",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}